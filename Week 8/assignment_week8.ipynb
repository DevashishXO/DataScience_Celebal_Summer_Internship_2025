{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c995ff8e",
   "metadata": {},
   "source": [
    "## **Assignment - Week 8**\n",
    "\n",
    "**Problem Statement** \n",
    "\n",
    "Build a RAG Q&A chatbot using document retrieval and generative AI for intelligent response generation.\n",
    "\n",
    "**Resources**\n",
    "\n",
    "[Kaggle] https://www.kaggle.com/datasets/sonalisingh1411/loan-approval-prediction?select=Training+Dataset.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50deff0",
   "metadata": {},
   "source": [
    "### **Step 1: Data Preprocessing**\n",
    "\n",
    "In this step, we will find which columns have missing values. We will also fill them with reasonable defaults (either mode or median or a placeholder).We will also convert all relevant fields to strings to avoid errors like `.lower()` on floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d4657e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Loan_ID Gender Married Dependents     Education Self_Employed  \\\n",
      "0  LP001002   Male      No          0      Graduate            No   \n",
      "1  LP001003   Male     Yes          1      Graduate            No   \n",
      "2  LP001005   Male     Yes          0      Graduate           Yes   \n",
      "3  LP001006   Male     Yes          0  Not Graduate            No   \n",
      "4  LP001008   Male      No          0      Graduate            No   \n",
      "\n",
      "   ApplicantIncome  CoapplicantIncome  LoanAmount  Loan_Amount_Term  \\\n",
      "0             5849                0.0         NaN             360.0   \n",
      "1             4583             1508.0       128.0             360.0   \n",
      "2             3000                0.0        66.0             360.0   \n",
      "3             2583             2358.0       120.0             360.0   \n",
      "4             6000                0.0       141.0             360.0   \n",
      "\n",
      "   Credit_History Property_Area Loan_Status  \n",
      "0             1.0         Urban           Y  \n",
      "1             1.0         Rural           N  \n",
      "2             1.0         Urban           Y  \n",
      "3             1.0         Urban           Y  \n",
      "4             1.0         Urban           Y  \n",
      "Loan_ID               0\n",
      "Gender               13\n",
      "Married               3\n",
      "Dependents           15\n",
      "Education             0\n",
      "Self_Employed        32\n",
      "ApplicantIncome       0\n",
      "CoapplicantIncome     0\n",
      "LoanAmount           22\n",
      "Loan_Amount_Term     14\n",
      "Credit_History       50\n",
      "Property_Area         0\n",
      "Loan_Status           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"rag_chatbot/data/train.csv\")\n",
    "print(df.head())\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6261069c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling categorical columns with mode\n",
    "for col in ['Gender', 'Married', 'Dependents', 'Self_Employed', 'Credit_History']:\n",
    "    df[col] = df[col].fillna(df[col].mode()[0])\n",
    "\n",
    "# Filling numerical columns with median\n",
    "for col in ['LoanAmount', 'Loan_Amount_Term']:\n",
    "    df[col] = df[col].fillna(df[col].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4fc8bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting all relevant fields to strings to avoid errors\n",
    "for col in ['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', \n",
    "            'Property_Area', 'Loan_Status']:\n",
    "    df[col] = df[col].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7a0b5e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Loan ID LP002473 was applied by a married male graduate with 0 dependents. The applicant has an income of â‚¹8334 and co-applicant income of â‚¹0.0. The loan amount requested was â‚¹160.0 with a term of 360.0 months. The credit history is good. The property area is semiurban. The loan was not approved.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def row_to_text(row):\n",
    "    return (\n",
    "        f\"Loan ID {row['Loan_ID']} was applied by \"\n",
    "        f\"{'a married' if row['Married'] == 'Yes' else 'an unmarried'} \"\n",
    "        f\"{row['Gender'].lower()} \"\n",
    "        f\"{'graduate' if row['Education'] == 'Graduate' else 'not graduate'} \"\n",
    "        f\"with {row['Dependents']} dependents. \"\n",
    "        f\"The applicant has an income of â‚¹{row['ApplicantIncome']} and \"\n",
    "        f\"co-applicant income of â‚¹{row['CoapplicantIncome']}. \"\n",
    "        f\"The loan amount requested was â‚¹{row['LoanAmount']} with a term of {row['Loan_Amount_Term']} months. \"\n",
    "        f\"The credit history is {'good' if row['Credit_History'] == 1.0 else 'bad'}. \"\n",
    "        f\"The property area is {row['Property_Area'].lower()}. \"\n",
    "        f\"The loan was {'approved' if row['Loan_Status'] == 'Y' else 'not approved'}.\"\n",
    "    )\n",
    "\n",
    "df[\"doc_text\"] = df.apply(row_to_text, axis=1)\n",
    "# Sample text from the dataframe\n",
    "df[\"doc_text\"].sample(1).values[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea0a183",
   "metadata": {},
   "source": [
    "### **Step 2: Embedding + FAISS Index Creation (using Sentence Transformers)**  \n",
    "\n",
    "We are going to generate dense embeddings for each document chunk using a `Sentence Transformer`. We will also store these embeddings in a FAISS index for fast similarity search during user queries.\n",
    "Fianally, we will save the index and mapping for use in the chatbot.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "968b1a55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10ee861384354124a9fdf588a1e22320",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Getting text chunks\n",
    "docs = df[\"doc_text\"].tolist()\n",
    "\n",
    "# Generating embeddings (using SentenceTransformer to convert texts to vectors)\n",
    "embeddings = model.encode(docs, show_progress_bar=True)\n",
    "\n",
    "# Converting to float32 as required by FAISS\n",
    "embeddings = np.array(embeddings).astype(\"float32\")\n",
    "\n",
    "# Initializing FAISS index (using a simple flat index for cosine similarity)\n",
    "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "\n",
    "# Adding embeddings to the index\n",
    "index.add(embeddings)\n",
    "faiss.write_index(index, \"rag_chatbot/vector_store/index.faiss\")\n",
    "\n",
    "# Saving document texts for retrieving the sources later\n",
    "with open(\"rag_chatbot/vector_store/id2doc.pkl\", \"wb\") as f:\n",
    "    pickle.dump(docs, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dc7b90",
   "metadata": {},
   "source": [
    "### **Step 3: Retriever Module (Top-k Similarity Search)**\n",
    "\n",
    "We now build the `retriever.py` utility to retrieve the most relevant document chunks for any user query.  \n",
    "The query is embedded using the same Sentence Transformer, then matched against our FAISS index to return the top-k most similar documents.\n",
    "\n",
    "This is the \"R\" in the RAG pipeline: fast, semantic document retrieval before generation.\n",
    "\n",
    "Kindly have a look at `retriever.py` in the `rag_chatbot` folder for the implementation details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5429e252",
   "metadata": {},
   "source": [
    "### **Step 4: Testing the Retriever Module**\n",
    "\n",
    "We will now test the `DocumentRetriever` on a sample user query.  \n",
    "The system embeds the query, searches the FAISS index, and returns the top-k most relevant document chunks.\n",
    "\n",
    "This verifies the \"retrieval\" part of our RAG pipeline is working correctly before moving to generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7acd76bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ Match #1 (Score: 0.8799999952316284):\n",
      "Loan ID LP001846 was applied by an unmarried female graduate with 3+ dependents. The applicant has an income of â‚¹3083 and co-applicant income of â‚¹0.0. The loan amount requested was â‚¹255.0 with a term of 360.0 months. The credit history is good. The property area is rural. The loan was approved.\n",
      "\n",
      "ðŸ”¹ Match #2 (Score: 0.8700000047683716):\n",
      "Loan ID LP001888 was applied by an unmarried female graduate with 0 dependents. The applicant has an income of â‚¹3237 and co-applicant income of â‚¹0.0. The loan amount requested was â‚¹30.0 with a term of 360.0 months. The credit history is good. The property area is urban. The loan was approved.\n",
      "\n",
      "ðŸ”¹ Match #3 (Score: 0.8600000143051147):\n",
      "Loan ID LP001788 was applied by an unmarried female graduate with 0 dependents. The applicant has an income of â‚¹3463 and co-applicant income of â‚¹0.0. The loan amount requested was â‚¹122.0 with a term of 360.0 months. The credit history is good. The property area is urban. The loan was approved.\n"
     ]
    }
   ],
   "source": [
    "from rag_chatbot.retriever import DocumentRetriever\n",
    "\n",
    "retriever = DocumentRetriever(\n",
    "    index_path=\"rag_chatbot/vector_store/index.faiss\",\n",
    "    id2doc_path=\"rag_chatbot/vector_store/id2doc.pkl\"\n",
    ")\n",
    "\n",
    "sample_query = \"What kind of applicants get approved for a loan?\"\n",
    "top_k = 3\n",
    "\n",
    "# Retrieving top-k relevant chunks\n",
    "results = retriever.retrieve(sample_query, top_k=top_k)\n",
    "results = sorted(results, key=lambda x: x[1], reverse=True) # Just to ensure results are sorted by score\n",
    "for i, (doc, score) in enumerate(results, 1):\n",
    "    print(f\"\\nðŸ”¹ Match #{i} (Score: {round(score, 2)}):\\n{doc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6401d3",
   "metadata": {},
   "source": [
    "### **Step 5: Answer Generation**\n",
    "\n",
    "We now build a **generator module** to produce natural language answers using the user query and the top-k retrieved context documents. The input to the generator is a concatenated prompt including the query and documents, which is passed to a small, efficient model from Hugging Face. Please refer to `rag_chatbot/generator.py` for the implementation details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea890d33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e8b222086f94a00af92a797ddc0fa8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\HP\\.cache\\huggingface\\hub\\models--google--flan-t5-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bacd8212a1d9480895ff2178df2ca4e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/308M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad573e40897a4b5b954e7c9128e1ba4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be53581a0dfd4886b2a43e269f0dc7a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64e7f735a9ff42ffa1372fd78d3e10b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(â€¦)a5b18a05535c9e14c7a355904270e15b0945ea86:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dae6663d582f45319bee978ae4e0457b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2706d9a4c55a4f7fa18869b4d767e1be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§  Generated Answer:\n",
      " unmarried female graduate with 0 dependents\n"
     ]
    }
   ],
   "source": [
    "# Testing the generator module\n",
    "from rag_chatbot.generator import AnswerGenerator\n",
    "\n",
    "# Using the retrieved chunks from earlier\n",
    "retrieved_chunks = [doc for doc, score in results]\n",
    "\n",
    "gen = AnswerGenerator()\n",
    "generated_answer = gen.generate_answer(sample_query, retrieved_chunks)\n",
    "\n",
    "print(\"ðŸ§  Generated Answer:\\n\", generated_answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44bebd9",
   "metadata": {},
   "source": [
    "### **Step 6: Building the RAG Chatbot Interface**\n",
    "\n",
    "We will now create an interactive chatbot using `Gradio`, designed to answer user queries based on training loan application records. It uses Retrieval-Augmented Generation:\n",
    "\n",
    "1. A user submits a query.\n",
    "2. The retriever fetches top-k relevant document chunks from a FAISS index.\n",
    "3. The generator uses those chunks + the userâ€™s query to produce a context-aware answer.\n",
    "\n",
    "The chatbot interface will:\n",
    "- Accept queries\n",
    "- Display the generated answer\n",
    "- Show the retrieved source chunks\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
